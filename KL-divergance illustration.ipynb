{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, expon, lognorm, entropy, weibull_min\n",
    "\n",
    "class KLModelEvaluator:\n",
    "    def __init__(self, observed_data, bins=40):\n",
    "        self.observed_data = observed_data\n",
    "        self.bins = np.histogram_bin_edges(observed_data, bins=bins)\n",
    "        self.observed_hist, _ = np.histogram(observed_data, bins=self.bins, density=True)\n",
    "        self.observed_midpoints = (self.bins[:-1] + self.bins[1:]) / 2\n",
    "        self.models = {}\n",
    "        self.kl_results = {}\n",
    "\n",
    "    def add_model(self, name, distribution, fit_args=None):\n",
    "        fit_args = fit_args or {}\n",
    "        params = distribution.fit(self.observed_data, **fit_args)\n",
    "        pdf = distribution.pdf(self.observed_midpoints, *params)\n",
    "        self.models[name] = pdf\n",
    "        self.kl_results[name] = self._kl_divergence(self.observed_hist, pdf)\n",
    "\n",
    "    def _kl_divergence(self, p, q):\n",
    "        epsilon = 1e-10\n",
    "        p = np.clip(p, epsilon, None)\n",
    "        q = np.clip(q, epsilon, None)\n",
    "        return entropy(p, q)\n",
    "\n",
    "    def plot_results(self):\n",
    "        num_models = len(self.models)\n",
    "        rows = (num_models + 1) // 2\n",
    "        fig, axes = plt.subplots(rows, 2, figsize=(12, 5 * rows))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, (name, pdf) in enumerate(self.models.items()):\n",
    "            axes[i].hist(self.observed_data, bins=self.bins, density=True, alpha=0.6, color=\"skyblue\", label=\"Observed Distribution\")\n",
    "            axes[i].plot(self.observed_midpoints, pdf, 'r-', label=f\"{name} Distribution\")\n",
    "            axes[i].set_title(f\"KL Divergence: {self.kl_results[name]:.2f} ({name})\")\n",
    "            axes[i].set_xlabel(\"Value\")\n",
    "            axes[i].set_ylabel(\"Density\")\n",
    "            axes[i].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def print_results(self):\n",
    "        print(\"KL Divergence Results:\")\n",
    "        for name, kl in self.kl_results.items():\n",
    "            print(f\"{name}: {kl:.2f}\")\n",
    "        best_model = min(self.kl_results, key=self.kl_results.get)\n",
    "        print(f\"\\nThe most aligned model is: {best_model} with KL Divergence = {self.kl_results[best_model]:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Generate observed data (complex simulated data)\n",
    "    np.random.seed(42)\n",
    "    data1 = np.random.normal(loc=5, scale=1.5, size=500)\n",
    "    data2 = np.random.exponential(scale=2.0, size=300)\n",
    "    data3 = np.random.uniform(low=2, high=6, size=200)\n",
    "    observed_data = np.concatenate([data1, data2, data3])\n",
    "\n",
    "    # Initialize evaluator\n",
    "    evaluator = KLModelEvaluator(observed_data)\n",
    "\n",
    "    # Add models\n",
    "    evaluator.add_model(\"Gaussian\", norm)\n",
    "    evaluator.add_model(\"Exponential\", expon)\n",
    "    evaluator.add_model(\"Log-Normal\", lognorm, fit_args={\"floc\": 0})\n",
    "    evaluator.add_model(\"Weibull\", weibull_min, fit_args={\"floc\": 0})\n",
    "\n",
    "    # Plot results\n",
    "    evaluator.plot_results()\n",
    "\n",
    "    # Print KL divergence results\n",
    "    evaluator.print_results()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
